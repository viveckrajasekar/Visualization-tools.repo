export PYSPARK_PYTHON=/usr/local/bin/python2.7

Make sure we have PySpark, HIVE, MYSQL and SQOOP

- Start mysql service

sudo su mysql
service mysqld start

-Create a database. 
-Create tables

CREATE TABLE `iris_data` (
  `sepal_length` decimal(10,2) DEFAULT NULL,
  `sepal_width` decimal(10,2) DEFAULT NULL,
  `petal_length` decimal(10,2) DEFAULT NULL,
  `petal_width` decimal(10,2) DEFAULT NULL,
  `class` varchar(200) DEFAULT NULL
) 

-Insert into the tables 

- Start hive metastore service
	
hive --service metastore

- Start HIVE

create database;

create external table, so that we can modify the location any time.

 create EXTERNAL TABLE `external_iris_data`(
  `sepal_length` string, 
  `sepal_width` string, 
  `petal_length` string, 
  `petal_width` string, 
  `class` string)
row format delimited
fields terminated by ','
stored as textfile


Start SQOOP

sqoop import --connect jdbc:mysql://localhost/test --username root --password root -table iris_data -m 1 --direct;

alter table external_iris_data set location "hdfs://localhost:54310/user/hduser/iris_data"

hadoop fs -ls hdfs://localhost:54310/user/hduser/iris_data

Start SPARK Cluster:

start-master.sh

start-slaves.sh

export PYSPARK_PYTHON=/usr/local/bin/python2.7

pyspark



